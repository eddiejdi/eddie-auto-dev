# Quick Reference: Pipeline Dual-GPU

## âœ… Status Atual
- **GPU0 (RTX 2060)**: Processing final â†’ Ativa
- **GPU1 (GTX 1050)**: Context preprocessing â†’ Ativa
- **Proxy**: http://192.168.15.2:8512 â†’ Operacional

## ğŸ§ª Testar Manualmente

### Verifique GPU utilization
```bash
ssh homelab@192.168.15.2 "watch -n 1 nvidia-smi"
```

### Monitorar logs em tempo real
```bash
ssh homelab@192.168.15.2 "sudo journalctl -u llm-optimizer -f"
```

### Teste direto ao proxy
```bash
# Pequeno contexto (< 2K tokens) â†’ GPU0
curl -X POST http://192.168.15.2:8512/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5-coder:7b",
    "messages": [{"role": "user", "content": "O que Ã© Python?"}],
    "stream": true
  }' | head -20

# Grande contexto (> 2K tokens) â†’ GPU1 + GPU0
# (adaptando seu contexto maior)
```

## ğŸ“Š Como Funciona

### Fluxo de DecisÃ£o no Proxy
```
Request chega com stream=true
    â†“
Estima tokens = len(messages) / 4
    â†“
â”œâ”€ < 2K tokens    â†’ Direto GPU0 (rÃ¡pido)
â”œâ”€ 2-6K tokens    â†’ GPU1 sumariza + GPU0 responde
â””â”€ > 6K tokens    â†’ GPU1 map-reduce + GPU0 responde
```

### Portas Ollama
- **:11434** - GPU0 (RTX 2060) qwen2.5-coder:7b
- **:11435** - GPU1 (GTX 1050) qwen3:1.7b

## ğŸ”§ Ajustes

### Mudar thresholds (arquivo proxy)
```bash
ssh homelab@192.168.15.2
# Edit /home/homelab/llm-optimizer/llm_optimizer.py
STRATEGY_A_MAX = 2000    # tokens â†’ GPU0 direto
STRATEGY_B_MAX = 6000    # tokens â†’ GPU1 preprocess
# > STRATEGY_B_MAX â†’ map-reduce
```

### Reiniciar proxy apÃ³s mudanÃ§as
```bash
ssh homelab@192.168.15.2 "sudo systemctl restart llm-optimizer"
```

## ğŸš€ PrÃ³ximas Features

- [ ] Teste com Cline (VS Code)
- [ ] Monitorar latÃªncia real
- [ ] Expandir para iGPU (Intel Iris)
- [ ] Dashboard Prometheus com mÃ©tricas dual-GPU

## ğŸ“ Logs Importantes

Verifique se vocÃª vÃª estas linhas nos logs:
```
/api/chat: direto GPU0 (X tokens, stream=True) â†’ estratÃ©gia rÃ¡pida
/api/chat: dual-GPU pipeline B (X tokens) â†’ GPU1 + GPU0
/api/chat: map-reduce pipeline C (X tokens) â†’ GPU1 chunks + GPU0
```

## âš¡ Performance Esperada

- **< 2K tokens**: ~2-3 seg (GPU0 direto)
- **2-6K tokens**: ~5-8 seg (GPU1 + GPU0)
- **> 6K tokens**: ~10-15 seg (map-reduce)

(Tempos variam com modelo e hardware)

## ğŸ› Troubleshooting

| Problema | SoluÃ§Ã£o |
|----------|---------|
| GPU1 nÃ£o usa memÃ³ria | Verifique thresholds, teste com > 2K tokens |
| Respostas lentas | Ajuste STRATEGY_A_MAX/STRATEGY_B_MAX |
| Proxy nÃ£o responde | `sudo systemctl restart llm-optimizer` |
| GPU out of memory | Reduzir STRATEGY_B_MAX ou MAX_CTX_SIZE |

---

**Last Updated**: 2026-03-01 13:30 UTC
