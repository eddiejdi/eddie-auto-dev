# ‚úÖ NVIDIA GPU + OLLAMA - CONFIGURA√á√ÉO CONCLU√çDA

**Data**: 27 de Fevereiro de 2026  
**GPU**: NVIDIA GeForce RTX 2060 SUPER  
**Driver**: 580.126.09  
**CUDA**: 13.0  
**Compute Capability**: 7.5 (Turing)  
**CPU**: Intel Core i9-9900T (8 cores / 16 threads)  

## üìä Status

### ‚úì Hardware Detectado
```
GPU: NVIDIA GeForce RTX 2060 SUPER
Memory: 8192 MiB
Bus ID: 00000000:02:00.0
PCI Slot: 02:00.0
Power Limit: 175W
```

### ‚úì Software Instalado
- Driver NVIDIA 580.126.09
- CUDA Toolkit 13.0
- Ollama com suporte CUDA
- Kernel: 6.8.0-101 (com `iommu=off`)

## üéØ Modelos Dispon√≠veis

Os seguintes modelos est√£o carregados em Ollama e **agora usam GPU**:
- `qwen2.5-coder:7b` (4.6 GB)
- `qwen3:14b` (9.2 GB)

## üöÄ Como Usar

### 1. Teste R√°pido com CURL
```bash
# No seu computador local
ssh homelab@192.168.15.2
curl -s http://localhost:11434/api/generate \
  -d '{"model":"qwen2.5-coder:7b","prompt":"1+1=","stream":false}' | python3 -m json.tool
```

### 2. Monitorar GPU em Tempo Real
```bash
# Execute no seu computador
bash /home/edenilson/eddie-auto-dev/monitor_gpu_ollama.sh
```

### 3. Verificar Se GPU Est√° Sendo Used
```bash
# No homelab
nvidia-smi

# Deve mostrar processo `ollama` usando GPU se modelo estiver rodando
```

## ‚öôÔ∏è Configura√ß√µes Aplicadas

### Systemd Drop-ins para Ollama
```
/etc/systemd/system/ollama.service.d/
‚îî‚îÄ‚îÄ ollama-optimized.conf   # Config √∫nica consolidada
```

### Vari√°veis de Ambiente
```bash
# CPU Threading (i9-9900T: cores 0-1 reservados para SO)
CPUAffinity=2-15                # 14 threads dispon√≠veis
GGML_NUM_THREADS=10             # 10 threads compute
OMP_PROC_BIND=spread            # Distribui uniformemente entre cores
OMP_PLACES=threads              # Cada thread em HW thread diferente
GOMAXPROCS=6                    # Go runtime: 6 cores f√≠sicos

# GPU (RTX 2060 SUPER 8GB - Turing CC 7.5)
CUDA_VISIBLE_DEVICES=0          # Use GPU 0
OLLAMA_NUM_GPU=999              # Offload m√°ximo para VRAM
OLLAMA_FLASH_ATTENTION=true     # Otimizar attention

# Inference
OLLAMA_NUM_PARALLEL=2           # 2 requests simult√¢neos
OLLAMA_CONTEXT_LENGTH=32768     # Contexto estendido (Cline)
OLLAMA_KV_CACHE_TYPE=q8_0       # Cache quantizado
OLLAMA_MAX_LOADED_MODELS=1      # 1 modelo na VRAM
OLLAMA_KEEP_ALIVE=30m           # Manter modelo 30min
```

### Kernel Parameters
```bash
# GRUB: /etc/default/grub
GRUB_CMDLINE_LINUX_DEFAULT=" pci=realloc iommu=off"
```

## üìà Performance Esperado

Com RTX 2060 SUPER (8GB VRAM):
- **VRAM**: ~7.5 GB utiliz√°veis (modelos at√© 14B Q4_K_M cabem)
- **Inference Speed**: ~31 tokens/sec (qwen2.5-coder:7b), ~20 tok/s (qwen3:14b)
- **CPU Distribution**: uniforme em cores 2-15 (ondas sim√©tricas no btop)
- **Latency**: ~200-500ms por token (modelos locais com GPU offload)

## üîß Troubleshooting

### GPU n√£o aparece em `nvidia-smi`
```bash
# No homelab, como root:
sudo systemctl restart ollama
nvidia-smi
sudo dmesg | tail -20 | grep -i nvidia
```

### Ollama n√£o usa GPU mesmo configurado
```bash
# For√ßar detec√ß√£o:
sudo systemctl stop ollama
unset CUDA_VISIBLE_DEVICES
sudo systemctl start ollama

# Verificar logs:
journalctl -u ollama -f
```

### Muita lat√™ncia (GPU n√£o acelerando)
```bash
# Verificar se GPU est√° em uso:
nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv,noheader
# Se 0% utilization durante infer√™ncia = problema
# Tente modelo menor:
curl -s http://localhost:11434/api/tags | python3 -m json.tool
```

## üìÅ Arquivos de Refer√™ncia

- [install_nvidia_cuda.sh](install_nvidia_cuda.sh) - Script de instala√ß√£o
- [monitor_gpu_ollama.sh](monitor_gpu_ollama.sh) - Monitor de GPU
- [GTX1050_SUCCESS.txt](GTX1050_SUCCESS.txt) - Relat√≥rio de sucesso

## üéì Pr√≥ximos Passos

1. **Fine-tune modelos**: Use GPU para treinamento de embeddings
2. **Multi-model inference**: Carregue m√∫ltiplos modelos simultaneamente
3. **Batch processing**: Processe v√°rios prompts em paralelo
4. **Model optimization**: Quantizar modelos para melhor performance

---

**Status**: ‚úÖ PRONTO PARA PRODU√á√ÉO  
**√öltima atualiza√ß√£o**: 2026-02-28 02:30 UTC  
**Config**: ollama-optimized.conf (consolidado) ‚Äî CPU spread + GPU offload
