# ğŸ¤– Sistema LLM para Matching de Vagas

Sistema completo de compatibilidade entre currÃ­culos e vagas usando **LLM (eddie-whatsapp)** com **fine-tuning automÃ¡tico** baseado em feedback.

---

## ğŸ“‹ VisÃ£o Geral

### Problema Original
- **MÃ©todo Jaccard**: simples token matching, score muito baixo (max 1.1% em vagas reais)
- **LimitaÃ§Ãµes**: nÃ£o entende sinÃ´nimos (K8s â‰  Kubernetes), nÃ£o considera contexto semÃ¢ntico

### SoluÃ§Ã£o Implementada
1. **LLM Semantic Matching**: usa modelo eddie-whatsapp para anÃ¡lise semÃ¢ntica
2. **Hybrid Scoring**: 70% LLM + 30% Jaccard para balancear semÃ¢ntica e keywords
3. **Training Data Collection**: coleta automÃ¡tica de feedback (emails enviados, aceitos, rejeitados)
4. **Auto Fine-tuning**: re-treina modelo com dados coletados para melhorar precisÃ£o

---

## ğŸš€ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    apply_real_job.py                        â”‚
â”‚  (Pipeline principal: WhatsApp â†’ CurrÃ­culo â†’ Email)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         llm_compatibility.py                       â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚  â”‚ LLM Only     â”‚ Jaccard Only â”‚ Hybrid (70/30) â”‚ â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                         â”‚
            â–¼                         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Ollama API       â”‚      â”‚ Training Data        â”‚
  â”‚ eddie-whatsapp   â”‚      â”‚ Collector            â”‚
  â”‚ :11434           â”‚      â”‚ (SQLite)             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ Fine-tuning          â”‚
                            â”‚ (JSONL â†’ Modelfile)  â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Componentes

### 1. `llm_compatibility.py`
**Funcionalidades:**
- `compute_compatibility_llm()`: scoring com LLM puro
- `compute_compatibility_hybrid()`: 70% LLM + 30% Jaccard
- `compute_compatibility_fallback()`: Jaccard tradicional (backup)
- `benchmark_compatibility_methods()`: compara todos os mÃ©todos

**Exemplo de uso:**
```python
from llm_compatibility import compute_compatibility_hybrid

score, explanation, details = compute_compatibility_hybrid(resume_text, job_text)
# score: 85.3
# explanation: "LLM: 92.0%, Jaccard: 68.5%, Final: 85.3% | Strong match: Kubernetes..."
# details: {"llm_score": 92.0, "jaccard_score": 68.5, "method": "hybrid"}
```

### 2. `training_data_collector.py`
**Funcionalidades:**
- `init_training_db()`: cria SQLite database `/tmp/whatsapp_training.db`
- `collect_training_sample()`: armazena cada comparaÃ§Ã£o
- `update_training_feedback()`: atualiza com feedback manual
- `export_training_dataset()`: gera JSONL para fine-tuning
- `show_training_dashboard()`: estatÃ­sticas (MAE, accuracy, etc.)

**Tabela SQLite:**
```sql
CREATE TABLE training_samples (
    id INTEGER PRIMARY KEY,
    timestamp TEXT,
    resume_text TEXT,
    job_text TEXT,
    predicted_score REAL,
    actual_score REAL,
    user_feedback TEXT,
    was_sent INTEGER,
    email_status TEXT,
    llm_explanation TEXT,
    jaccard_score REAL,
    method TEXT
)
```

### 3. `finetune_whatsapp_model.py`
**Funcionalidades:**
- `load_training_data()`: carrega JSONL dataset
- `create_modelfile()`: gera Modelfile com few-shot examples
- `create_model_via_api()`: cria modelo via Ollama API
- `validate_model()`: testa modelo apÃ³s fine-tuning
- `compare_model_versions()`: compara original vs fine-tuned

**Workflow:**
1. Carrega `whatsapp_training_dataset.jsonl` (mÃ­nimo 10 samples)
2. Cria Modelfile com system prompt + few-shot examples
3. Chama Ollama API `/api/create` para treinar
4. Valida modelo com teste de sanidade
5. Compara performance: base model vs fine-tuned

### 4. `apply_real_job.py` (integrado)
**MudanÃ§as:**
- `compute_compatibility()` retorna `(score, explanation, details)` tuple
- Coleta automÃ¡tica de training samples
- Log detalhado de LLM vs Jaccard scores
- Fallback automÃ¡tico se LLM falhar

---

## âš™ï¸ VariÃ¡veis de Ambiente

```bash
# LLM Configuration
export USE_LLM_COMPATIBILITY=1          # 1=enabled, 0=disabled
export COMPATIBILITY_METHOD=hybrid      # llm, jaccard, or hybrid
export COMPATIBILITY_THRESHOLD=1.0      # Score threshold (0-100)
export COLLECT_TRAINING_DATA=1          # 1=collect, 0=no collect

# Ollama
export OLLAMA_HOST=http://192.168.15.2:11434
export WHATSAPP_MODEL=eddie-whatsapp:latest

# Training
export TRAINING_DB=/tmp/whatsapp_training.db
```

---

## ğŸ¯ Uso

### Modo Interativo (Recomendado)
```bash
python3 llm_system_demo.py
```

Menu:
1. **Test LLM Compatibility** - comparar mÃ©todos (Jaccard vs LLM vs Hybrid)
2. **Collect Training Data** - coletar 5-10 amostras simuladas
3. **Show Training Dashboard** - mÃ©tricas (MAE, scores mÃ©dios, etc.)
4. **Export Training Dataset** - gerar JSONL para fine-tuning
5. **Fine-tune Model** - re-treinar eddie-whatsapp
6. **Run Full Pipeline** - executar apply_real_job.py com LLM
7. **Exit**

### Uso Direto

#### 1. Testar compatibilidade (benchmark)
```bash
python3 llm_compatibility.py
```
Mostra 3 exemplos comparando Jaccard vs LLM vs Hybrid.

#### 2. Coletar dados de treino (manual)
```bash
export COLLECT_TRAINING_DATA=1
export DEMO_MODE=1
export COMPATIBILITY_THRESHOLD=60.0

# Executar 10x para coletar samples
for i in {1..10}; do
    python3 apply_real_job.py
done
```

#### 3. Ver estatÃ­sticas de treino
```bash
python3 training_data_collector.py stats
```
Output:
```
ğŸ“Š DASHBOARD DE TREINAMENTO
   Total de amostras: 127
   Emails enviados: 18
   Amostras com feedback: 5
   Amostras com correÃ§Ã£o: 3
   
   Score mÃ©dio predito: 62.5%
   Score mÃ©dio real: 58.3%
   Erro absoluto mÃ©dio: 4.2%
   
   Melhoria do LLM sobre Jaccard: +45.2%
```

#### 4. Exportar dataset para fine-tuning
```bash
python3 training_data_collector.py export
```
Gera `/tmp/whatsapp_training_dataset.jsonl` com formato:
```json
{"prompt": "VocÃª Ã© um especialista...\nCURRÃCULO: ...\nVAGA: ...", "completion": "Score: 85%\nJustificativa: Match excelente..."}
```

#### 5. Fazer fine-tuning
```bash
python3 finetune_whatsapp_model.py
```
Workflow:
- Carrega training dataset
- Cria Modelfile com few-shot examples
- Chama Ollama API para criar modelo
- Valida com teste
- Exibe comparaÃ§Ã£o antes/depois

#### 6. Usar no pipeline de produÃ§Ã£o
```bash
export USE_LLM_COMPATIBILITY=1
export COMPATIBILITY_METHOD=hybrid
export COMPATIBILITY_THRESHOLD=1.0
export COLLECT_TRAINING_DATA=1

python3 apply_real_job.py
```

---

## ğŸ“Š ComparaÃ§Ã£o de MÃ©todos

### Jaccard (Baseline)
```
Vaga: "SRE para trabalhar com K8s e cloud AWS"
CurrÃ­culo: "DevOps Engineer com Kubernetes, Docker, AWS"
Score: 15.2%  âŒ (nÃ£o reconhece K8s = Kubernetes)
```

### LLM Only
```
Vaga: "SRE para trabalhar com K8s e cloud AWS"
CurrÃ­culo: "DevOps Engineer com Kubernetes, Docker, AWS"
Score: 88.5%  âœ… (entende sinÃ´nimos e contexto)
ExplicaÃ§Ã£o: "Match excelente: experiÃªncia em Kubernetes (K8s), AWS, perfil DevOps/SRE compatÃ­vel"
```

### Hybrid (70% LLM + 30% Jaccard)
```
Vaga: "SRE para trabalhar com K8s e cloud AWS"
CurrÃ­culo: "DevOps Engineer com Kubernetes, Docker, AWS"
LLM: 88.5% | Jaccard: 15.2% â†’ Final: 66.5%  âœ… (balanceado)
```

**RecomendaÃ§Ã£o:** `hybrid` oferece melhor balanceamento entre anÃ¡lise semÃ¢ntica e keyword matching.

---

## ğŸ”§ Fine-tuning Workflow

### 1. Coletar Dados (â‰¥10 amostras)
```bash
# Modo automÃ¡tico (demo)
python3 llm_system_demo.py  # OpÃ§Ã£o 2

# Modo manual (vagas reais)
export COLLECT_TRAINING_DATA=1
python3 run_auto_apply.py 10
```

### 2. Adicionar Feedback Manual (opcional)
```python
from training_data_collector import update_training_feedback

# CorreÃ§Ã£o de score
update_training_feedback(sample_id=15, user_feedback="good_match", actual_score=85.0)

# Apenas feedback qualitativo
update_training_feedback(sample_id=16, user_feedback="bad_match (spam)")
```

### 3. Exportar Dataset
```bash
python3 training_data_collector.py export
```

### 4. Fine-tune
```bash
python3 finetune_whatsapp_model.py
```

### 5. Validar Resultado
```bash
# Comparar antes/depois
python3 finetune_whatsapp_model.py compare

# Testar em produÃ§Ã£o
export WHATSAPP_MODEL=eddie-whatsapp:latest
python3 llm_compatibility.py
```

---

## ğŸ“ˆ MÃ©tricas e Monitoramento

### Training Dashboard
```bash
python3 training_data_collector.py stats
```
Mostra:
- Total de samples coletados
- Emails enviados
- Samples com feedback/correÃ§Ã£o
- Score mÃ©dio: predito vs real
- Mean Absolute Error (MAE)
- Melhoria do LLM sobre Jaccard

### Logs Detalhados
```bash
tail -f /tmp/email_logs/email_log.txt
```
Cada comparaÃ§Ã£o exibe:
```
2026-02-11 20:30:15 - INFO - ğŸ¤– LLM Hybrid: 68.5% (LLM: 72.0%, Jaccard: 58.5%)
2026-02-11 20:30:15 - INFO - Found match with compat 68.5%
```

---

## ğŸ“ Conceitos

### Jaccard Similarity
```
score = |A âˆ© B| / |A âˆª B| Ã— 100
```
Onde:
- A = conjunto de tokens do currÃ­culo
- B = conjunto de tokens da vaga
- âˆ© = interseÃ§Ã£o (palavras em comum)
- âˆª = uniÃ£o (todas as palavras Ãºnicas)

**LimitaÃ§Ã£o:** apenas overlap literal, sem semÃ¢ntica.

### LLM Semantic Matching
- Usa modelo de linguagem (eddie-whatsapp) para entender **significado**
- Reconhece sinÃ´nimos (Kubernetes = K8s)
- Considera contexto (SRE â‰ˆ DevOps â‰ˆ Platform Engineer)
- Avalia senioridade (Pleno vs SÃªnior)
- Ignora diferenÃ§as irrelevantes (idioma, formato)

### Hybrid Approach
```
final_score = (llm_score Ã— 0.7) + (jaccard_score Ã— 0.3)
```
Combina:
- **SemÃ¢ntica** (LLM): entende significado e contexto
- **Keywords** (Jaccard): garante overlap de termos tÃ©cnicos exatos

**Vantagem:** mais robusto - se LLM errar, Jaccard corrige; se Jaccard falhar (sinÃ´nimos), LLM compensa.

### Few-shot Learning
Fine-tuning adiciona exemplos no Modelfile:
```
### Exemplo 1:
USER: [prompt com currÃ­culo DevOps + vaga SRE]
ASSISTANT: Score: 85%\nJustificativa: Match excelente, experiÃªncia K8s/AWS...

### Exemplo 2:
USER: [prompt com currÃ­culo DevOps + vaga Data Science]
ASSISTANT: Score: 12%\nJustificativa: Ãreas diferentes, mÃ­nimo overlap...
```
Modelo aprende padrÃµes de scoring corretos.

---

## ğŸ› Troubleshooting

### Erro: "LLM compatibility not available"
```bash
# Verificar mÃ³dulos
python3 -c "from llm_compatibility import compute_compatibility_llm"

# Verificar Ollama
curl http://192.168.15.2:11434/api/tags
```

### Erro: "Model eddie-whatsapp not found"
```bash
# Listar modelos
curl -s http://192.168.15.2:11434/api/tags | python3 -m json.tool

# Criar modelo se necessÃ¡rio
python3 finetune_whatsapp_model.py
```

### Scores muito baixos (LLM + Jaccard)
- **Causa:** grupos do WhatsApp nÃ£o contÃªm vagas relevantes
- **SoluÃ§Ã£o:** entrar em grupos de DevOps/SRE/Platform Engineering ou ajustar threshold para 0.5-2.0%

### Fine-tuning falha: "Only X samples available"
- **Causa:** precisa â‰¥10 samples com correÃ§Ã£o para treino significativo
- **SoluÃ§Ã£o:** coletar mais dados via `python3 llm_system_demo.py` opÃ§Ã£o 2

### LLM timeout
- **Causa:** Ollama sobrecarregado ou modelo muito pesado
- **SoluÃ§Ã£o:** aguardar ou usar modelo menor (ex: `dolphin-llama3:8b` â†’ `dolphin-llama3:3b`)

---

## ğŸ“š PrÃ³ximos Passos

### Melhorias PossÃ­veis
1. **TF-IDF weighting**: dar mais peso a termos tÃ©cnicos raros
2. **Named Entity Recognition (NER)**: extrair tecnologias especÃ­ficas
3. **Embeddings**: usar sentence-transformers para similaridade vetorial
4. **Active Learning**: priorizar samples com alta incerteza para feedback
5. **A/B Testing**: comparar modelos em produÃ§Ã£o com split 50/50

### Escalabilidade
- Mover training DB para PostgreSQL (multi-process)
- Cache de embeddings para re-scoring rÃ¡pido
- Batch processing para mÃºltiplas vagas simultÃ¢neas
- API REST para scoring externo

---

## ğŸ“– ReferÃªncias

- [Ollama API Docs](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Jaccard Similarity](https://en.wikipedia.org/wiki/Jaccard_index)
- [Few-shot Learning](https://arxiv.org/abs/2005.14165)
- [Fine-tuning LLMs](https://huggingface.co/docs/transformers/training)

---

## ğŸ¤ Contribuindo

Para adicionar novo mÃ©todo de scoring:

1. Implementar em `llm_compatibility.py`:
```python
def compute_compatibility_new_method(resume_text, job_text):
    # Seu algoritmo aqui
    return score, explanation, details
```

2. Adicionar opÃ§Ã£o em `apply_real_job.py`:
```python
if COMPATIBILITY_METHOD == "new_method":
    score, explanation, details = compute_compatibility_new_method(resume_text, job_text)
```

3. Testar com benchmark:
```python
python3 llm_compatibility.py
```

---

## ğŸ“„ LicenÃ§a

MIT License - use livremente, atribua crÃ©ditos.

---

**ğŸ‘¤ Autor:** Eddie Auto-Dev Team  
**ğŸ“… Data:** Fevereiro 2026  
**ğŸ”§ VersÃ£o:** 1.0.0
