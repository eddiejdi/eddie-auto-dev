# üîß Ollama Recovery Report - 2026-02-28

## üî¥ Problema Detectado

- **Status**: Ollama travado (congelado)
- **Sintomas**: 
  - GPU: 0% de utiliza√ß√£o
  - VRAM: 7.4GB alocados (sem processamento)
  - API: Respondendo mas n√£o processando
- **Hor√°rio**: 2026-02-28 18:00 UTC
- **Causa Raiz**: Deadlock em goroutine do runner Ollama (processo em espera indefinida)

## üìã A√ß√µes Realizadas

### 1. ‚úÖ Restart Imediato (18:00-18:01)
```bash
sudo systemctl restart ollama
```
**Resultado**: GPU voltou a 30% de utiliza√ß√£o, 17 modelos carregados, API responsiva

### 2. ‚úÖ Instala√ß√£o dos Daemons Permanentes (18:01-18:02)

Scripts criados em commit anterior (`13d3d91`) mas **nunca instalados como servi√ßos**:

```bash
scp tools/selfheal/ollama_frozen_monitor.sh homelab@192.168.15.2:/tmp/
scp tools/selfheal/ollama_metrics_exporter.sh homelab@192.168.15.2:/tmp/
sudo mv /tmp/ollama_frozen_monitor.sh /usr/local/bin/ollama_frozen_monitor
sudo mv /tmp/ollama_metrics_exporter.sh /usr/local/bin/ollama_metrics_exporter
sudo chmod +x /usr/local/bin/ollama_*
```

### 3. ‚úÖ Cria√ß√£o dos Servi√ßos Systemd (18:02)

**ollama-frozen-monitor.service**:
- Detec√ß√£o autom√°tica de congelamento
- Trigger: GPU < 5% AND nenhuma requisi√ß√£o por > 180s
- Auto-restart via `sudo systemctl restart ollama`
- M√°ximo 3 restarts/hora, cooldown 60s entre tentativas
- Logs: `journalctl -u ollama-frozen-monitor`

**ollama-metrics-exporter.service**:
- Coleta m√©tricas do Ollama a cada 15s
- Exporta para Prometheus em `/tmp/ollama_metrics.prom`
- M√©tricas: `ollama_up`, `ollama_frozen_duration_seconds`, `ollama_models_loaded`, GPU utilization, etc.

### 4. ‚úÖ Resolu√ß√£o de Permiss√µes (18:02-18:03)

Problemas iniciais:
- User=ollama n√£o tinha permiss√£o para escrever em `/var/log/` e `/tmp/`
- Necess√°rio User=root para `sudo systemctl restart ollama`

Solu√ß√£o:
- Alterado User=ollama ‚Üí User=root em ambos os servi√ßos
- Limpeza de conflitos: `rm -f /tmp/ollama_*.{txt,json,prom}`
- Reinicializa√ß√£o dos servi√ßos

## ‚úÖ Verifica√ß√£o Final

```
üìä Status do Ollama:
   - Modelos carregados: 17
   - API: respondendo
   - GPU: 0% (ocioso, esperado)
   - VRAM: 4.9GB / 8.2GB

üöÄ Servi√ßos de Monitoramento:
   - ollama-frozen-monitor: active (running)
   - ollama-metrics-exporter: active (running)
   - Auto-start at boot: enabled

üìà M√©tricas Sendo Exportadas:
   - /tmp/ollama_metrics.prom (2.1K)
   - /tmp/ollama_metrics.txt (578B)
```

## üõ°Ô∏è Auto-Recovery Agora Ativado

| Par√¢metro | Valor |
|-----------|-------|
| Threshold de congelamento | 180 segundos |
| GPU m√≠n. esperado | 5% |
| Intervalo de check | 15 segundos |
| Max restarts/hora | 3 |
| Cooldown entre tentativas | 60 segundos |
| Comportamento | Auto-restart + logging |

## üéØ Por Que o Auto-Recovery Falhou Antes?

**Root cause**: Os scripts de monitoramento foram criados e commitados, mas **nunca foi feita a instala√ß√£o como servi√ßos systemd**. 

Mudan√ßas necess√°rias que faltavam:
1. Copiar scripts para `/usr/local/bin/`
2. Criar arquivos de servi√ßo em `/etc/systemd/system/`
3. `systemctl daemon-reload`
4. `systemctl enable` e `systemctl start`

Agora est√° corrigido e operacional.

## üìù Pr√≥ximos Passos

- [ ] Verificar no Grafana se as gauges de Ollama est√£o exibindo dados
- [ ] Testar auto-recovery com simula√ß√£o de congelamento (ver `SELFHEALING_SETUP.md`)
- [ ] Validar alertas Prometheus para `OllamaFrozen`
- [ ] Monitorar logs: `journalctl -u ollama-frozen-monitor -f`

## üìö Refer√™ncia

Documenta√ß√£o completa: [SELFHEALING_SETUP.md](./SELFHEALING_SETUP.md)

Scripts envolvidos:
- `tools/selfheal/ollama_frozen_monitor.sh` - Detec√ß√£o + restart
- `tools/selfheal/ollama_metrics_exporter.sh` - Coleta de m√©tricas
- `monitoring/prometheus/selfhealing_rules.yml` - Alerting rules

---

**Report generated**: 2026-02-28 18:03 UTC  
**Status**: ‚úÖ RESOLVED (Auto-recovery now operational)
