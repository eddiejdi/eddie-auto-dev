#!/usr/bin/env python3
"""
ğŸ¯ RAG AI Dashboard - Painel de Monitoramento da IA
Monitora desempenho, acurÃ¡cia e mÃ©tricas do sistema RAG
"""

import streamlit as st
import requests
import json
import time
from datetime import datetime, timedelta

# ConfiguraÃ§Ã£o
RAG_API = "http://192.168.15.2:8001/api/v1"
OLLAMA_API = "http://192.168.15.2:11434/api"

st.set_page_config(
    page_title="ğŸ¯ RAG AI Dashboard",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS Customizado
st.markdown("""
<style>
    .metric-card {
        background: linear-gradient(135deg, #1e3a5f 0%, #2d5a87 100%);
        padding: 20px;
        border-radius: 15px;
        color: white;
        text-align: center;
        margin: 10px 0;
    }
    .metric-value {
        font-size: 2.5em;
        font-weight: bold;
    }
    .metric-label {
        font-size: 0.9em;
        opacity: 0.8;
    }
    .status-online {
        color: #00ff00;
        font-weight: bold;
    }
    .status-offline {
        color: #ff0000;
        font-weight: bold;
    }
    .collection-badge {
        background: #4CAF50;
        color: white;
        padding: 5px 15px;
        border-radius: 20px;
        margin: 5px;
        display: inline-block;
    }
</style>
""", unsafe_allow_html=True)

def check_service(url, name):
    """Verifica se um serviÃ§o estÃ¡ online"""
    try:
        r = requests.get(url, timeout=5)
        return r.status_code == 200
    except:
        return False

def get_rag_stats():
    """ObtÃ©m estatÃ­sticas do RAG"""
    try:
        r = requests.get(f"{RAG_API}/rag/stats", timeout=10)
        if r.status_code == 200:
            return r.json()
    except:
        pass
    return None

def get_ollama_models():
    """Lista modelos do Ollama"""
    try:
        r = requests.get(f"{OLLAMA_API}/tags", timeout=10)
        if r.status_code == 200:
            return r.json().get('models', [])
    except:
        pass
    return []

def test_rag_search(query, collection="default"):
    """Testa uma busca no RAG"""
    try:
        start = time.time()
        r = requests.post(
            f"{RAG_API}/rag/search",
            json={"query": query, "n_results": 3, "collection": collection},
            timeout=30
        )
        latency = (time.time() - start) * 1000
        if r.status_code == 200:
            return r.json(), latency
    except:
        pass
    return None, 0

def test_ollama_inference(prompt, model="llama3.2"):
    """Testa inferÃªncia do Ollama"""
    try:
        start = time.time()
        r = requests.post(
            f"{OLLAMA_API}/generate",
            json={"model": model, "prompt": prompt, "stream": False},
            timeout=60
        )
        latency = (time.time() - start) * 1000
        if r.status_code == 200:
            return r.json(), latency
    except:
        pass
    return None, 0

# =============================================================================
# SIDEBAR
# =============================================================================
with st.sidebar:
    st.title("ğŸ¯ RAG Dashboard")
    st.markdown("---")
    
    # Status dos ServiÃ§os
    st.subheader("ğŸ“¡ Status dos ServiÃ§os")
    
    rag_online = check_service(f"{RAG_API.replace('/api/v1', '')}/health", "RAG")
    ollama_online = check_service(f"{OLLAMA_API}/tags", "Ollama")
    
    col1, col2 = st.columns(2)
    with col1:
        if rag_online:
            st.success("âœ… RAG API")
        else:
            st.error("âŒ RAG API")
    with col2:
        if ollama_online:
            st.success("âœ… Ollama")
        else:
            st.error("âŒ Ollama")
    
    st.markdown("---")
    
    # NavegaÃ§Ã£o
    st.subheader("ğŸ“Š NavegaÃ§Ã£o")
    page = st.radio("", [
        "ğŸ  VisÃ£o Geral",
        "ğŸ“š Collections",
        "ğŸ” Teste de Busca",
        "ğŸ§  Teste de InferÃªncia",
        "ğŸ“ˆ Benchmark"
    ])
    
    st.markdown("---")
    st.caption(f"Ãšltima atualizaÃ§Ã£o: {datetime.now().strftime('%H:%M:%S')}")
    if st.button("ğŸ”„ Atualizar"):
        st.rerun()

# =============================================================================
# PÃGINAS
# =============================================================================

if page == "ğŸ  VisÃ£o Geral":
    st.title("ğŸ  VisÃ£o Geral do Sistema")
    
    stats = get_rag_stats()
    
    if stats:
        # MÃ©tricas principais
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                label="ğŸ“„ Total de Documentos",
                value=stats.get('total_documents', 0),
                delta="+26 (Bitcoin)"
            )
        
        with col2:
            st.metric(
                label="ğŸ’¬ Conversas Indexadas",
                value=stats.get('total_conversations', 0)
            )
        
        with col3:
            st.metric(
                label="ğŸ“ Collections",
                value=len(stats.get('collections', []))
            )
        
        with col4:
            feedback_rate = stats.get('positive_feedback_rate', 0)
            st.metric(
                label="ğŸ‘ Taxa de Feedback Positivo",
                value=f"{feedback_rate:.0%}" if feedback_rate else "N/A"
            )
        
        st.markdown("---")
        
        # Collections
        st.subheader("ğŸ“ Collections DisponÃ­veis")
        collections = stats.get('collections', [])
        
        cols = st.columns(len(collections) if collections else 1)
        for i, coll in enumerate(collections):
            with cols[i]:
                icon = "ğŸª™" if coll == "bitcoin_knowledge" else "ğŸ“š" if coll == "chat_history" else "ğŸ“"
                st.info(f"{icon} **{coll}**")
        
        # Ãšltimo aprendizado
        st.markdown("---")
        st.subheader("ğŸ• Ãšltima ExecuÃ§Ã£o de Aprendizado")
        last_run = stats.get('last_learning_run', 'N/A')
        if last_run != 'N/A':
            try:
                dt = datetime.fromisoformat(last_run.replace('Z', '+00:00'))
                st.success(f"âœ… {dt.strftime('%d/%m/%Y Ã s %H:%M:%S')}")
            except:
                st.info(last_run)
    else:
        st.error("âŒ NÃ£o foi possÃ­vel obter estatÃ­sticas do RAG")

elif page == "ğŸ“š Collections":
    st.title("ğŸ“š Detalhes das Collections")
    
    stats = get_rag_stats()
    if stats:
        collections = stats.get('collections', [])
        
        for coll in collections:
            with st.expander(f"ğŸ“ {coll}", expanded=(coll == "bitcoin_knowledge")):
                # Teste de busca na collection
                test_result, latency = test_rag_search("teste", coll)
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("âš¡ LatÃªncia", f"{latency:.0f}ms")
                with col2:
                    if test_result:
                        n_results = len(test_result.get('results', []))
                        st.metric("ğŸ“Š Resultados de Teste", n_results)
                
                if coll == "bitcoin_knowledge":
                    st.success("ğŸª™ Esta collection contÃ©m conhecimento especializado em Bitcoin!")
                    st.markdown("""
                    **TÃ³picos cobertos:**
                    - Fundamentos do Bitcoin
                    - Blockchain e tecnologia
                    - MineraÃ§Ã£o e Halving
                    - Carteiras e seguranÃ§a
                    - Lightning Network
                    - Taproot e SegWit
                    - ETFs e mercado
                    """)

elif page == "ğŸ” Teste de Busca":
    st.title("ğŸ” Teste de Busca RAG")
    
    stats = get_rag_stats()
    collections = stats.get('collections', ['default']) if stats else ['default']
    
    col1, col2 = st.columns([3, 1])
    with col1:
        query = st.text_input("Digite sua pergunta:", placeholder="Ex: O que Ã© Bitcoin?")
    with col2:
        collection = st.selectbox("Collection:", collections)
    
    n_results = st.slider("NÃºmero de resultados:", 1, 10, 3)
    
    if st.button("ğŸ” Buscar", type="primary"):
        if query:
            with st.spinner("Buscando..."):
                result, latency = test_rag_search(query, collection)
            
            if result:
                st.success(f"âœ… Busca concluÃ­da em {latency:.0f}ms")
                
                results = result.get('results', [])
                if results:
                    for i, r in enumerate(results):
                        with st.expander(f"ğŸ“„ Resultado {i+1}", expanded=(i==0)):
                            content = r.get('content', 'N/A')
                            st.markdown(content[:1000] + "..." if len(content) > 1000 else content)
                            
                            metadata = r.get('metadata', {})
                            if metadata:
                                st.caption(f"ğŸ“Œ TÃ³pico: {metadata.get('topic', 'N/A')} | Fonte: {metadata.get('source', 'N/A')}")
                else:
                    st.warning("âš ï¸ Nenhum resultado encontrado")
            else:
                st.error("âŒ Erro na busca")
        else:
            st.warning("Digite uma pergunta")

elif page == "ğŸ§  Teste de InferÃªncia":
    st.title("ğŸ§  Teste de InferÃªncia Ollama")
    
    models = get_ollama_models()
    model_names = [m.get('name', 'unknown') for m in models] if models else ['llama3.2']
    
    col1, col2 = st.columns([3, 1])
    with col1:
        prompt = st.text_area("Digite seu prompt:", placeholder="Ex: Explique o que Ã© Bitcoin em 3 frases.", height=100)
    with col2:
        model = st.selectbox("Modelo:", model_names)
    
    if st.button("ğŸš€ Gerar", type="primary"):
        if prompt:
            with st.spinner(f"Gerando com {model}..."):
                result, latency = test_ollama_inference(prompt, model)
            
            if result:
                st.success(f"âœ… Gerado em {latency/1000:.1f}s")
                
                response = result.get('response', 'N/A')
                st.markdown("### ğŸ“ Resposta:")
                st.markdown(response)
                
                # MÃ©tricas
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("âš¡ LatÃªncia", f"{latency/1000:.1f}s")
                with col2:
                    tokens = result.get('eval_count', 0)
                    st.metric("ğŸ”¢ Tokens", tokens)
                with col3:
                    if tokens and latency:
                        tps = tokens / (latency/1000)
                        st.metric("ğŸ“ˆ Tokens/s", f"{tps:.1f}")
            else:
                st.error("âŒ Erro na inferÃªncia")
        else:
            st.warning("Digite um prompt")

elif page == "ğŸ“ˆ Benchmark":
    st.title("ğŸ“ˆ Benchmark de Performance")
    
    st.markdown("Execute testes automatizados para medir a performance do sistema.")
    
    if st.button("ğŸš€ Iniciar Benchmark Completo", type="primary"):
        progress = st.progress(0)
        status = st.empty()
        
        results = {
            "rag_searches": [],
            "ollama_inferences": []
        }
        
        # Benchmark RAG
        test_queries = [
            ("O que Ã© Bitcoin?", "bitcoin_knowledge"),
            ("Como funciona a blockchain?", "bitcoin_knowledge"),
            ("O que Ã© Lightning Network?", "bitcoin_knowledge"),
            ("Quem Ã© Satoshi Nakamoto?", "bitcoin_knowledge"),
            ("O que Ã© halving?", "bitcoin_knowledge"),
        ]
        
        status.info("ğŸ” Testando buscas RAG...")
        for i, (q, coll) in enumerate(test_queries):
            _, latency = test_rag_search(q, coll)
            results["rag_searches"].append({"query": q, "latency": latency})
            progress.progress((i + 1) / (len(test_queries) + 3) * 100 / 100)
        
        # Benchmark Ollama
        status.info("ğŸ§  Testando inferÃªncia Ollama...")
        test_prompts = [
            "Responda em uma frase: O que Ã© Bitcoin?",
            "Diga apenas sim ou nÃ£o: Bitcoin Ã© descentralizado?",
            "Complete: A blockchain do Bitcoin Ã©..."
        ]
        
        for i, p in enumerate(test_prompts):
            _, latency = test_ollama_inference(p)
            results["ollama_inferences"].append({"prompt": p[:30], "latency": latency})
            progress.progress((len(test_queries) + i + 1) / (len(test_queries) + 3) * 100 / 100)
        
        progress.progress(100)
        status.success("âœ… Benchmark concluÃ­do!")
        
        # Resultados
        st.markdown("---")
        st.subheader("ğŸ“Š Resultados")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### ğŸ” RAG Search")
            rag_latencies = [r['latency'] for r in results['rag_searches']]
            avg_rag = sum(rag_latencies) / len(rag_latencies) if rag_latencies else 0
            
            st.metric("LatÃªncia MÃ©dia", f"{avg_rag:.0f}ms")
            st.metric("LatÃªncia MÃ­nima", f"{min(rag_latencies):.0f}ms" if rag_latencies else "N/A")
            st.metric("LatÃªncia MÃ¡xima", f"{max(rag_latencies):.0f}ms" if rag_latencies else "N/A")
            
            # ClassificaÃ§Ã£o
            if avg_rag < 100:
                st.success("ğŸ† Excelente performance!")
            elif avg_rag < 300:
                st.info("âœ… Boa performance")
            else:
                st.warning("âš ï¸ Performance pode ser melhorada")
        
        with col2:
            st.markdown("### ğŸ§  Ollama Inference")
            ollama_latencies = [r['latency'] for r in results['ollama_inferences']]
            avg_ollama = sum(ollama_latencies) / len(ollama_latencies) if ollama_latencies else 0
            
            st.metric("LatÃªncia MÃ©dia", f"{avg_ollama/1000:.1f}s")
            st.metric("LatÃªncia MÃ­nima", f"{min(ollama_latencies)/1000:.1f}s" if ollama_latencies else "N/A")
            st.metric("LatÃªncia MÃ¡xima", f"{max(ollama_latencies)/1000:.1f}s" if ollama_latencies else "N/A")
            
            # ClassificaÃ§Ã£o
            if avg_ollama < 2000:
                st.success("ğŸ† Excelente performance!")
            elif avg_ollama < 5000:
                st.info("âœ… Boa performance")
            else:
                st.warning("âš ï¸ Performance pode ser melhorada")
        
        # Detalhes
        st.markdown("---")
        st.subheader("ğŸ“‹ Detalhes dos Testes")
        
        with st.expander("ğŸ” Detalhes RAG"):
            for r in results['rag_searches']:
                st.write(f"- **{r['query']}**: {r['latency']:.0f}ms")
        
        with st.expander("ğŸ§  Detalhes Ollama"):
            for r in results['ollama_inferences']:
                st.write(f"- **{r['prompt']}...**: {r['latency']/1000:.1f}s")

# Footer
st.markdown("---")
st.caption("ğŸ¤– RAG AI Dashboard v1.0 | Monitoramento em tempo real do sistema de IA")
