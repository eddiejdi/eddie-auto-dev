#!/usr/bin/env python3
"""
ReviewAgent ‚Äî Agente especializado em Quality Gate + CI/CD Review

Responsabilidades:
1. Validar c√≥digo (estilo, seguran√ßa, duplica√ß√£o, complexidade)
2. Executar testes (unit, E2E com Selenium, integra√ß√£o com outros agents)
3. Gerar/validar documenta√ß√£o (Confluence, Jira, Draw.io)
4. Rejeitar commits ruins com feedback claro
5. Aprovar commits bons
6. Retrospectiva e aprendizado dos agentes
7. Recomenda√ß√µes de refatora√ß√£o

Usa modelo LLM grande (33B+) para an√°lise profunda.
"""
import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from enum import Enum

from .base_agent import LLMClient
from .agent_communication_bus import (
    get_communication_bus, MessageType,
    log_task_start, log_task_end, log_error
)

logger = logging.getLogger(__name__)


class ReviewStatus(Enum):
    PENDING = "pending"
    IN_REVIEW = "in_review"
    REJECTED = "rejected"
    APPROVED = "approved"
    MERGED = "merged"


class ReviewDecision(Enum):
    """Resultado da an√°lise"""
    APPROVE = "approve"
    REJECT = "reject"
    REQUEST_CHANGES = "request_changes"
    NEEDS_RETEST = "needs_retest"


class ReviewAgent:
    """
    Agente Review especializado.
    Modelo: Claude 3.5 Sonnet (via Ollama 70B+ ou API externa)
    """

    def __init__(self):
        self.name = "review_agent"
        self.llm = LLMClient(model="claude-sonnet")  # Modelo grande
        self.system_prompt = self._build_system_prompt()
        self.decisions_log: List[Dict[str, Any]] = []

    def _build_system_prompt(self) -> str:
        """Prompt especializado para review de alta qualidade"""
        return """Voc√™ √© o ReviewAgent, especialista em Quality Gate e CI/CD da plataforma RPA4ALL.

RESPONSABILIDADES:
1. **Valida√ß√£o de C√≥digo**: arquitetura, padr√µes, seguran√ßa, performance
2. **Detec√ß√£o de Duplica√ß√£o**: encontrar commits iguais/similares
3. **Testes**: avaliar cobertura, requerer testes E2E quando necess√°rio
4. **Documenta√ß√£o**: validar Confluence, Jira, Draw.io atualizado
5. **Aprendizado**: identificar padr√µes ruins dos agentes e treinar
6. **Retrospectiva**: comparar qualidade antes/depois do treinamento

REGRAS DE DECIS√ÉO:
‚úÖ APROVAR se:
- C√≥digo segue padr√µes do projeto
- Sem duplica√ß√£o ou Copy-Paste
- Testes cobrem >80% ou justificado
- Documenta√ß√£o atualizada
- N√£o quebra pipelines existentes
- Commits bem estruturados (n√£o triviais/duplicados)

‚ùå REJEITAR se:
- C√≥digo duplicado/similar a PR anterior
- Teste falha ou n√£o existe para l√≥gica cr√≠tica
- Seguran√ßa em risco (hardcoded secrets, SQL injection, etc)
- Performance degradada (>10% mais lento)
- Documenta√ß√£o desatualizada
- Commits triviais (import, formatting sem mudan√ßa funcional)

üîÑ REQUERER MUDAN√áAS se:
- Design pode melhorar (mas funciona)
- Recomenda√ß√µes de refatora√ß√£o (n√£o bloqueia)
- Testes podem ser mais robustos

‚ö†Ô∏è RETESTE se:
- Testes falhram (pode ser flaky)
- Integra√ß√£o com outros agents necess√°ria
- Ambiente de CI inconsistente

SA√çDA OBRIGAT√ìRIA (JSON):
{
  "decision": "approve|reject|request_changes|needs_retest",
  "score": 0-100,
  "Summary": "resumo executivo (2-3 linhas)",
  "findings": ["achado 1", "achado 2", ...],
  "risks": ["risco 1 se houver"],
  "recommendations": ["recomenda√ß√£o 1", ...],
  "training_feedback": {
    "agent": "agent_name",
    "issue": "descri√ß√£o do padr√£o ruim",
    "training": "recomenda√ß√£o de treinamento"
  },
  "tests_required": ["test_type_1", "test_type_2"],
  "retry_count": 0
}
"""

    async def review_commit(
        self,
        commit_id: str,
        branch: str,
        author_agent: str,
        diff: str,
        files_changed: List[str],
        test_results: Optional[Dict] = None,
        previous_reviews: Optional[List[Dict]] = None,
    ) -> Dict[str, Any]:
        """
        Revisa um commit/PR completo.

        Args:
            commit_id: hash do commit
            branch: nome da branch
            author_agent: agent que criou
            diff: diff do Git
            files_changed: arquivos modificados
            test_results: resultados de testes (se rodados)
            previous_reviews: reviews anteriores para context (cyclical learning)

        Returns:
            Decision completa com detalhes
        """
        logger.info("üîç Iniciando review de %s por %s", commit_id[:7], author_agent)

        review = {
            "commit_id": commit_id,
            "branch": branch,
            "author_agent": author_agent,
            "reviewed_at": datetime.now().isoformat(),
            "status": ReviewStatus.IN_REVIEW.value,
        }

        try:
            # 1. An√°lise de duplica√ß√£o
            dup_score = await self._check_duplication(
                diff, files_changed, previous_reviews or []
            )
            if dup_score > 0.8:
                review["decision"] = ReviewDecision.REJECT.value
                review["reason"] = "C√≥digo duplicado detectado"
                review["duplication_score"] = dup_score
                logger.warning("‚ö†Ô∏è  Duplica√ß√£o alto: %.1f%%", dup_score * 100)
                self.decisions_log.append(review)
                return review

            # 2. An√°lise de c√≥digo (seguran√ßa, padr√µes, etc)
            code_analysis = await self._analyze_code(diff, files_changed)
            review["code_analysis"] = code_analysis

            # 3. Valida√ß√£o de testes
            test_validation = await self._validate_tests(
                files_changed, test_results, code_analysis
            )
            review["test_validation"] = test_validation

            # 4. Verifica√ß√£o de documenta√ß√£o
            docs_check = await self._check_documentation(files_changed, branch)
            review["docs_check"] = docs_check

            # 5. Decis√£o final
            llm_prompt = self._build_review_prompt(
                diff, code_analysis, test_validation, docs_check, author_agent
            )
            decision_json = await self.llm.generate(
                llm_prompt, system=self.system_prompt, temperature=0.3
            )

            try:
                decision = json.loads(decision_json)
            except json.JSONDecodeError:
                # Fallback parsing
                decision = self._parse_review_fallback(decision_json)

            review["decision"] = decision.get("decision", "request_changes")
            review["score"] = decision.get("score", 50)
            review["summary"] = decision.get("summary", "Revis√£o conclu√≠da")
            review["findings"] = decision.get("findings", [])
            review["risks"] = decision.get("risks", [])
            review["recommendations"] = decision.get("recommendations", [])

            # 6. Aprendizado: registrar padr√£o ruim para treinar agent
            if decision.get("training_feedback"):
                await self._record_training_feedback(
                    author_agent, decision["training_feedback"]
                )

            # 7. Determinar testes necess√°rios
            review["required_tests"] = decision.get("tests_required", [])

            self.decisions_log.append(review)
            logger.info(
                "‚úÖ Review conclu√≠do: %s (score=%d)",
                decision["decision"],
                review["score"],
            )

        except Exception as e:
            logger.error("üí• Erro na review: %s", e)
            review["decision"] = "error"
            review["error"] = str(e)
            self.decisions_log.append(review)

        return review

    async def _check_duplication(
        self, diff: str, files: List[str], previous_reviews: List[Dict]
    ) -> float:
        """Detectar se commit √© duplicado vs anteriores (0.0-1.0)"""
        if not previous_reviews:
            return 0.0

        # An√°lise simples: hash do diff, compare com anteriores
        import hashlib

        current_hash = hashlib.sha256(diff[:500].encode()).hexdigest()
        dup_count = 0

        for prev in previous_reviews[-10:]:  # √öltimas 10 reviews
            prev_hash = hashlib.sha256(prev.get("diff", "")[:500].encode()).hexdigest()
            if current_hash == prev_hash:
                dup_count += 1

        return min(dup_count / max(len(previous_reviews), 1), 1.0)

    async def _analyze_code(self, diff: str, files: List[str]) -> Dict[str, Any]:
        """An√°lise profunda de c√≥digo"""
        prompt = f"""Analise este c√≥digo (diff) para:
1. Seguran√ßa (secrets, injection, etc)
2. Performance (loops, queries, memory)
3. Padr√µes (arquitetura, design patterns)
4. Legibilidade e manutenibilidade

Diff:
{diff[:2000]}

Arquivos: {files}

Retorne JSON:
{{"security": ["achado1"], "performance": ["achado1"], "patterns": ["achado1"], "readability": ["achado1"]}}
"""
        response = await self.llm.generate(prompt, temperature=0.2)
        try:
            return json.loads(response)
        except:
            return {"raw": response}

    async def _validate_tests(
        self, files: List[str], test_results: Optional[Dict], code_analysis: Dict
    ) -> Dict[str, Any]:
        """Validar se testes cobrem mudan√ßas"""
        if not test_results:
            return {"status": "no_tests", "required": True}

        critical_files = [f for f in files if "core" in f or "agent" in f]
        coverage = test_results.get("coverage", 0)

        return {
            "status": "validated",
            "coverage": coverage,
            "critical_files": critical_files,
            "ok": coverage > 0.75,
        }

    async def _check_documentation(self, files: List[str], branch: str) -> Dict:
        """Verificar se documenta√ß√£o foi atualizada"""
        doc_files = ["README.md", "docs/", ".md"]
        has_docs = any(
            doc in str(files).lower() for doc in doc_files
        ) or "docs" in branch.lower()

        return {"has_docs": has_docs, "required": not has_docs}

    def _build_review_prompt(
        self,
        diff: str,
        code_analysis: Dict,
        test_validation: Dict,
        docs_check: Dict,
        agent_name: str,
    ) -> str:
        """Construir prompt para LLM tomar decis√£o"""
        return f"""
COMMIT PARA REVIEW:
- Agent: {agent_name}
- Diff (primeiras 1000 chars): {diff[:1000]}
- An√°lise de c√≥digo: {json.dumps(code_analysis, ensure_ascii=False)}
- Valida√ß√£o de testes: {json.dumps(test_validation, ensure_ascii=False)}
- Docs atualizado: {docs_check['has_docs']}

Baseado nisso, tome uma decis√£o e retorne JSON com:
- decision (approve/reject/request_changes/needs_retest)
- score (0-100)
- summary
- findings
- risks
- recommendations
- training_feedback (se houver padr√£o ruim)
- tests_required
"""

    def _parse_review_fallback(self, text: str) -> Dict[str, Any]:
        """Parser fallback se LLM n√£o retornar JSON v√°lido"""
        decision_map = {
            "approve": "approve",
            "reject": "reject",
            "request": "request_changes",
            "retest": "needs_retest",
        }

        for key, val in decision_map.items():
            if key.lower() in text.lower():
                return {"decision": val, "score": 50, "summary": text[:200]}

        return {"decision": "request_changes", "score": 50, "summary": text[:200]}

    async def _record_training_feedback(self, agent_name: str, feedback: Dict):
        """Registrar feedback para treinar agent (via bus)"""
        bus = get_communication_bus()
        try:
            bus.publish(
                MessageType.REQUEST,
                "review_agent",
                f"{agent_name}",
                json.dumps(
                    {
                        "type": "training_feedback",
                        "issue": feedback.get("issue"),
                        "training": feedback.get("training"),
                    }
                ),
                {"action": "train_from_review"},
            )
            logger.info("üìö Training feedback enviado para %s", agent_name)
        except Exception as e:
            logger.error("Erro enviando feedback: %s", e)

    async def retrospective(
        self, agent_name: str, period_days: int = 7
    ) -> Dict[str, Any]:
        """
        Fazer retrospectiva: como o agent evoluiu?
        Compara qualidade antes vs depois de treinamento.
        """
        # Filtrar decisions do agent nos √∫ltimos N dias
        cutoff = datetime.fromisoformat(
            (datetime.now().timestamp() - period_days * 86400).__str__()
        )

        agent_reviews = [
            r
            for r in self.decisions_log
            if r.get("author_agent") == agent_name
            and datetime.fromisoformat(r.get("reviewed_at", "")) > cutoff
        ]

        if not agent_reviews:
            return {"agent": agent_name, "status": "no_reviews_in_period"}

        # Calcular m√©tricas
        avg_score = sum(r.get("score", 50) for r in agent_reviews) / len(agent_reviews)
        approved_pct = (
            sum(
                1
                for r in agent_reviews
                if r.get("decision") == ReviewDecision.APPROVE.value
            )
            / len(agent_reviews)
            * 100
        )
        dup_issues = sum(1 for r in agent_reviews if "duplication_score" in r)

        return {
            "agent": agent_name,
            "period_days": period_days,
            "reviews_count": len(agent_reviews),
            "avg_score": avg_score,
            "approved_pct": approved_pct,
            "duplication_issues": dup_issues,
            "trend": "improving" if len(agent_reviews) < 5 else "stable",
            "recommendations": [
                f"Agente melhorou em {approved_pct:.0f}% de aprova√ß√µes"
                if approved_pct > 70
                else "Agente precisa melhorar em qualidade",
                "Reduzir commit duplicados" if dup_issues > 2 else "Bom controle de duplica√ß√£o",
            ],
        }

    def get_status(self) -> Dict[str, Any]:
        """Status geral do ReviewAgent"""
        return {
            "name": self.name,
            "total_reviews": len(self.decisions_log),
            "approvals": sum(
                1 for r in self.decisions_log if r.get("decision") == "approve"
            ),
            "rejections": sum(
                1 for r in self.decisions_log if r.get("decision") == "reject"
            ),
            "avg_score": (
                sum(r.get("score", 50) for r in self.decisions_log)
                / max(len(self.decisions_log), 1)
            ),
        }
