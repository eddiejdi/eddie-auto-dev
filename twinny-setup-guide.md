# Guia de Configura√ß√£o do Twinny para Ollama Remoto

## Seu Servidor Ollama
- **Host:** 192.168.15.2
- **Porta:** 11434
- **Modelos dispon√≠veis:**
  - `codestral:22b` (22.2B par√¢metros)
  - `deepseek-coder-v2:16b` (15.7B par√¢metros)
  - `qwen2.5-coder:7b` (7.6B par√¢metros)
  - `qwen2.5-coder:1.5b` (1.5B par√¢metros)

---

## Configura√ß√£o via Interface do Twinny

### 1. Abrir Providers do Twinny
1. Clique no √≠cone do Twinny na barra lateral esquerda (ü§ñ)
2. No painel do Twinny, clique no √≠cone de engrenagem ‚öôÔ∏è
3. Navegue at√© a se√ß√£o "Providers"

### 2. Configurar Provider para Chat

Crie ou edite um provider com estas configura√ß√µes:

| Campo | Valor |
|-------|-------|
| **Label** | `Ollama Homelab Chat` |
| **Provider** | `ollama` |
| **Type** | `chat` |
| **Hostname** | `192.168.15.2` |
| **Port** | `11434` |
| **Path** | `/v1/chat/completions` |
| **Model** | `codestral:22b` (ou outro modelo) |
| **Protocol** | `http` |

### 3. Configurar Provider para Code Completion (FIM)

Crie outro provider para auto-complete:

| Campo | Valor |
|-------|-------|
| **Label** | `Ollama Homelab FIM` |
| **Provider** | `ollama` |
| **Type** | `fim` (Fill-in-Middle) |
| **Hostname** | `192.168.15.2` |
| **Port** | `11434` |
| **Path** | `/api/generate` |
| **Model** | `codestral:22b` |
| **Protocol** | `http` |
| **FIM Template** | `codestral` ou `deepseek` |

---

## Configura√ß√£o via settings.json

Adicione estas configura√ß√µes no seu `settings.json` (Ctrl+Shift+P ‚Üí "Preferences: Open User Settings (JSON)"):

```json
{
    "twinny.chatModelName": "codestral:22b",
    "twinny.fimModelName": "codestral:22b",
    "twinny.apiHostname": "192.168.15.2",
    "twinny.apiPort": 11434,
    "twinny.apiProtocol": "http",
    "twinny.apiPath": "/v1/chat/completions",
    "twinny.fimApiPath": "/api/generate",
    "twinny.apiProvider": "ollama",
    "twinny.enabled": true,
    "twinny.enabledChat": true,
    "twinny.enabledCodeActions": true,
    "twinny.autoSuggestEnabled": true
}
```

---

## Testar a Conex√£o

### Via Terminal (teste r√°pido):
```bash
# Testar endpoint de chat
curl http://192.168.15.2:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "codestral:22b",
    "messages": [{"role": "user", "content": "Hello"}]
  }'

# Testar endpoint de generate
curl http://192.168.15.2:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "codestral:22b",
    "prompt": "def hello():"
  }'
```

---

## Modelos Recomendados por Uso

| Uso | Modelo Recomendado | Por que |
|-----|-------------------|---------|
| **Chat (qualidade)** | `codestral:22b` | Maior, mais preciso |
| **Chat (velocidade)** | `qwen2.5-coder:7b` | Bom equil√≠brio |
| **FIM/Auto-complete** | `qwen2.5-coder:1.5b` | R√°pido para sugest√µes em tempo real |
| **Deep Coding** | `deepseek-coder-v2:16b` | Especializado em c√≥digo |

---

## Troubleshooting

### "Connection error"
1. Verifique se Ollama est√° rodando: `curl http://192.168.15.2:11434/api/tags`
2. Verifique firewall do servidor
3. Certifique-se que Ollama est√° configurado para aceitar conex√µes externas (`OLLAMA_HOST=0.0.0.0`)

### Modelo n√£o responde
1. Verifique se o modelo est√° baixado: `ollama list` no servidor
2. Tente com modelo menor primeiro (`qwen2.5-coder:1.5b`)

### Lento
1. Use modelos menores para FIM/auto-complete
2. Ajuste timeout nas configura√ß√µes do twinny
